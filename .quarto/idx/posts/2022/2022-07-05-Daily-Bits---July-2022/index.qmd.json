{"title":"Daily Bits - July 2022","markdown":{"yaml":{"author":"Sam White","toc-title":"Contents","toc-depth":5,"toc-location":"left","layout":"post","title":"Daily Bits - July 2022","date":"2022-07-05 09:49","tags":["daily bits","July"],"categories":["2022","Daily Bits"]},"headingText":"E P I D I V E R S E - S N P    P I P E L I N E","containsRefs":false,"markdown":"\n\n20220727\n\n- Ran qPCRs using 8 primer sets on Dorothy's cDNA from yesterday.\n\n---\n\n20220726\n\n- In lab to finish Dorothy's mussel RNA extraction and reverse transcription.\n\n\n---\n\n20220725\n\n- Read Ch. 7 of \"Fresh Banana Leaves\" for book club.\n\n- Lab meeting discussing Ch. 7 of \"Fresh Banana Leaves.\"\n\n---\n\n20220721\n\n- More testing with `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines (conda environments). This time ran trimming with an additional 5' 10bp hard trim, per [`Bismark`](https://github.com/FelixKrueger/Bismark) recommendations (they actually recommend 8bp).\n\n- Did a _lot_ of reading about Nextflow and trying to get a grasp on how it all works so we might be able to develop our own pipelines and/or feel confident modifying existing pipelines (e.g. modify `EpiDiverse/snp` pipeline to analyze all BAMs in a directory without the need to explicitly declare how many at run time).\n\n---\n\n20220720\n\n- More testing with `EpiDiverse/wgbs` pipeline. Per [this GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/11), I tried using the conda test profile and things ran smoothly without any errors in the `bam_statistics` part of the pipeline. Steps:\n\n1. Create conda environment:\n\n```shell\nconda env create --name epidiverse-wgbs-current -f /home/shared/epidiverse-pipelines/wgbs-current\n```\n\n2. Activate conda environment:\n\n```shell\nconda activate epdiverse-wgbs-current\n```\n\n3. Run conda test profile:\n\n```shell\nNXF_VER=20.07.1 /home/shared/nextflow run /home/shared/epidiverse-pipelines/wgbs-current -profile test,conda\n```\n\n- Did same conda test process for `EpiDiverse/snp` and it also ran without issue.\n\n- Started adding instructions for using Nextflow EpiDiverse pipelines on Raven and Mox to Roberts Lab Handbook.\n\n- Decided to re-run the `EpiDiverse/wgbs` pipeline using the conda environment and enable the FastQC process for post-trimming assessment. Will allow me to compare with our previous trimming results and try to determine why our trimming generates different results when passing our FastQs into the `EpiDiverse/snp` pipeline.\n\n---\n\n20220719\n\n- Assisted Dorothy with prep for big RNA extraction from mussel gills. Homogenized 10 control and 10 heat treated gill samples in 1mL RNAzol RT and stored @ -80C. Extraction to happen next Tuesday.\n\n---\n\n20220718\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220717\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220716\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220715\n\n- Managed to resolve the [`EpiDiverse/wgbs` test issue I encountered yesterday](https://github.com/EpiDiverse/wgbs/issues/9). See the link for details (involves binding local directory to Singularity container).\n\n- Successfully ran the `EpiDiverse/wgbs` pipeline on a subset of the _Ostrea lurida_ BSseq data! Now to test this in the `EpiDiverse/snp` pipeline to see if output is same as when running Bismark BAMs through the `EpiDiverse/snp` pipeline...\n\n- Tested Bismark BAMs from different data set (_Panopea generosa_: https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/1231/) in `EpiDiverse/snp` pipeline to see how results compare to the _Ostrea lurida_ Bismark BAMs.\n\n---\n\n20220714\n\n- Installed Singularity on Raven (required Go, so installed that, too).\n\n- Tried to run `EpiDiverse/wgbs` test run on Raven, but it failed. See [the GitHub Issue I posted for deets.](https://github.com/EpiDiverse/wgbs/issues/9)\n\n---\n\n20220713\n\n- Got response to [my `EpiDiverse/snp` GitHub issue](https://github.com/EpiDiverse/snp/issues/6) regarding the pipeline not processing all BAM files in directory! Will re-ran pipeline and add the `--take <INTEGER>` parameter to handle all 18 BAM files.\n\n- Fixed [issue with CEABIGR transcript count stats](https://github.com/RobertsLab/resources/issues/1490). Original code assumed that calculations were only performed on input data and did not process newly added data generated by `mutate()` function. Fixed that by using the `-any_of()` fucntion to skip a vector of column names. The `-` preceding the `any_of()` is the instruction to _exclude_ things entered in the `any_of()` function. \n\n\n---\n\n20220712\n\n- In lab: walked Dorothy through the basics of RNA isolation (RNAzol and Direct-zol Mini Kit) and quantification (Qubit RNA BR Assay). Processed two fresh mussel ctenidia samples.\n\n---\n\n20220711\n\n- Lab meeting: Discussed Ch.6 of Fresh Banana Leaves\n\n- Oyster gene expression meeting: Steven did some plotting of [_Crassostrea virginica_ (Eastern oyster)](https://en.wikipedia.org/wiki/Eastern_oyster) transcript count data.\n\n- Nextflow `epidiverse/snp`: opted to try [a subsetted version of the Olurida_v081 genome and BSseq alignments](https://github.com/RobertsLab/resources/issues/1489#issuecomment-1179224402) (GitHub Issue):\n\nLooks like it's working (got past the previous hangups on 20220708)!\n\n```\nN E X T F L O W  ~  version 20.07.1\nLaunching `epidiverse/snp` [sick_hoover] - revision: 9c814703c6 [master]\n\n================================================\n~ version 1.0\n\ninput dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/bams/070322-olymerge-snp\nreference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081-mergecat98.fa\noutput dir    : snps\nvariant calls : enabled\nclustering    : enabled\n\n================================================\nRUN NAME: sick_hoover\n\nexecutor >  local (60)\n[2c/3853d0] process > SNPS:preprocessing (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n[90/dea51f] process > SNPS:masking (zr1394_9_s456_trimmed_bismark_bt2.deduplicated - clustering) [100%] 20 of 20 ✔\n[8a/ce2b44] process > SNPS:extracting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)           [100%] 10 of 10 ✔\n[01/ef9503] process > SNPS:khmer (zr1394_6_s456_trimmed_bismark_bt2.deduplicated)                [ 40%] 4 of 10\n[-        ] process > SNPS:kwip                                                                  -\n[-        ] process > SNPS:clustering                                                            -\n[83/238cfa] process > SNPS:sorting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)              [100%] 10 of 10 ✔\n[03/1b910d] process > SNPS:freebayes (zr1394_7_s456_trimmed_bismark_bt2.deduplicated)            [  0%] 0 of 10\n[-        ] process > SNPS:bcftools                                                              -\n[-        ] process > SNPS:plot_vcfstats \n```\n\nWell, well, well! It worked!!\n\n```\nN E X T F L O W  ~  version 20.07.1\nLaunching `epidiverse/snp` [sick_hoover] - revision: 9c814703c6 [master]\n\n================================================\nE P I D I V E R S E - S N P    P I P E L I N E\n================================================\n~ version 1.0\n\ninput dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/bams/070322-olymerge-snp\nreference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081-mergecat98.fa\noutput dir    : snps\nvariant calls : enabled\nclustering    : enabled\n\n================================================\nRUN NAME: sick_hoover\n\nexecutor >  local (92)\n[2c/3853d0] process > SNPS:preprocessing (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n[90/dea51f] process > SNPS:masking (zr1394_9_s456_trimmed_bismark_bt2.deduplicated - clustering) [100%] 20 of 20 ✔\n[8a/ce2b44] process > SNPS:extracting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)           [100%] 10 of 10 ✔\n[a4/dc11ce] process > SNPS:khmer (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)                [100%] 10 of 10 ✔\n[95/79be9f] process > SNPS:kwip                                                                  [100%] 1 of 1 ✔\n[c6/24b89b] process > SNPS:clustering                                                            [100%] 1 of 1 ✔\n[83/238cfa] process > SNPS:sorting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)              [100%] 10 of 10 ✔\n[63/2c5bc5] process > SNPS:freebayes (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)            [100%] 10 of 10 ✔\n[00/f4af8b] process > SNPS:bcftools (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)             [100%] 10 of 10 ✔\n[df/1e78de] process > SNPS:plot_vcfstats (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n\nPipeline execution summary\n---------------------------\nName         : sick_hoover\nProfile      : docker\nLaunch dir   : /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse\nWork dir     : /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse/work (cleared)\nStatus       : success\nError report : -\n\nCompleted at: 11-Jul-2022 18:47:04\nDuration    : 3h 17m 49s\nCPU hours   : 140.1\nSucceeded   : 92\n\n```\n\nI'll obviously share this with Steven, but will also see if I can get the original dataset to run on Mox, which has twice the memory available as Raven...\n\nActually, I glanced at the data and it didn't really work. It only seems to have analyzed some of the BAMs in the directory! There are 18 BAMs, but it only processed 10 of them.. Odd (and annoying). I'll re-run to see if it does the same thing.\n\n\n---\n\n20220710\n\n- Created `erne-bs5` genome index to attempt full epidiverse pipeline, starting with `EpiDiverse/wgbs`:\n\n   ```shell\n/home/shared/erne-2.1.1-linux/bin/erne-create \\\n--methyl-hash \\\n--fasta Olurida_v081.fa \\\n--output-prefix Olurida_v081\n   ```\n- Realized I had experienced previous issues (see this [GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/5) and this [GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/6)) previously, which is not encouraging.\n\n  Tried to run the test protocol for this pipeline \"locally\" (i.e. offline) using Mox sbatch script. To do so, I've downloaded all the of the input files listed in [`test.config`](https://github.com/EpiDiverse/wgbs/blob/master/config/test.config). I've also downloaded the Singularity image (`singularity pull docker://epidiverse/wgbs:1.0`) and changed the `nextflow.config` file to specify the Singularity image location, like so:\n\n```\n// -profile singularity\n\tsingularity {\n\t\tincludeConfig \"${baseDir}/config/base.config\"\n\t\tsingularity.enabled = true\n\t\tprocess.container = '/gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/singularity/wgbs_1.0.sif'\n\t}\n```\n\nThat seemed like that should be all that was needed, but when I execute the test command (`NXF_VER=20.07.1 /gscratch/srlab/programs/nextflow-21.10.6-all run /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/wgbs-1.0 -profile test,singularity`), it fails with this error:\n\n```\nexecutor >  local (10)\n[c4/79070c] process > INDEX:erne_bs5_indexing        [100%] 1 of 1 ✔\n[30/202688] process > INDEX:segemehl_indexing        [100%] 1 of 1 ✔\n[07/dc2230] process > WGBS:read_trimming (sampleB)   [100%] 8 of 8, failed: 8...\n[-        ] process > WGBS:read_merging              -\n[-        ] process > WGBS:fastqc                    -\n[-        ] process > WGBS:erne_bs5                  -\n[-        ] process > WGBS:segemehl                  -\n[-        ] process > WGBS:erne_bs5_processing       -\n[-        ] process > WGBS:segemehl_processing       -\n[-        ] process > WGBS:bam_merging               -\n[-        ] process > WGBS:bam_subsetting            -\n[-        ] process > WGBS:bam_filtering             -\n[-        ] process > WGBS:bam_statistics            -\n[-        ] process > CALL:bam_processing            -\n[-        ] process > CALL:Picard_MarkDuplicates     -\n[-        ] process > CALL:MethylDackel              -\n[-        ] process > CALL:conversion_rate_estima... -\n\nPipeline execution summary\n---------------------------\nName         : infallible_mccarthy\nProfile      : test,singularity\nLaunch dir   : /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test\nWork dir     : /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work\nStatus       : failed\nError report : Error executing process > 'WGBS:read_trimming (sampleA)'\n\nCaused by:\n  Process `WGBS:read_trimming (sampleA)` terminated with an error exit status (1)\n\nCommand executed:\n\n  mkdir fastq fastq/logs\n  cutadapt -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC \\\n  -q 20 -m 36 -O 3 \\\n  -o fastq/merge.null \\\n  -p fastq/merge.g null g \\\n  > fastq/logs/cutadapt.sampleA.merge.log 2>&1\n\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n\nWork dir:\n  /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/12/6ee9cc7a7372a97f34f21a4f79efb3\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\nError executing process > 'WGBS:read_trimming (sampleA)'\n\nCaused by:\n  Process `WGBS:read_trimming (sampleA)` terminated with an error exit status (1)\n\nCommand executed:\n\n  mkdir fastq fastq/logs\n  cutadapt -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC \\\n  -q 20 -m 36 -O 3 \\\n  -o fastq/merge.null \\\n  -p fastq/merge.g null g \\\n  > fastq/logs/cutadapt.sampleA.merge.log 2>&1\n\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n\nWork dir:\n  /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/12/6ee9cc7a7372a97f34f21a4f79efb3\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n```\n\nWhen I look at the Cutadapt log file, this is what is shown:\n\n\n```\ncat cutadapt.sampleA.merge.log \nThis is cutadapt 2.10 with Python 3.6.7\nCommand line parameters: -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC -q 20 -m 36 -O 3 -o fastq/merge.null -p fastq/merge.g null g\nProcessing reads on 2 cores in paired-end mode ...\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 477, in run\n    with xopen(self.file, 'rb') as f:\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/xopen/__init__.py\", line 407, in xopen\n    return open(filename, mode)\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 477, in run\n    with xopen(self.file, 'rb') as f:\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/xopen/__init__.py\", line 407, in xopen\n    return open(filename, mode)\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 540, in run\n    raise e\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/bin/cutadapt\", line 10, in <module>\n    sys.exit(main())\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/__main__.py\", line 855, in main\n    stats = r.run()\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 770, in run\n    raise e\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n```\n\nI [posted an Issue](https://github.com/EpiDiverse/wgbs/issues/8), but I'm not really expecting to get a response.\n\n---\n\n20220709\n\n- Nextflow epidiverse/snp: Let it run overnight and woke up to the same timeout error message and kernel memory warnings, despite changes in config file.. Although, I had been running it with the `-resume` flag... I'll remove all previous `snps/` and `work/` dirs and run with the following command:\n\n  ```shell\n  NXF_VER=20.07.1 /home/shared/nextflow run epidiverse/snp \\\n  -c nextflow-docker_permissions.config \\\n  -profile docker \\\n  --input /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/ \\\n  --reference /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081.fa\n  ```\n\n  - Well that didn't work... I'm going to try again with a limited data set, as the `Olurida_v081.fa` has a TON of contigs, which has caused issues with other bisfulfite analysis software.\n\n---\n\n20220708\n\n- Spent time at Science Hour with Steven visualizing some of the [_Ostrea lurida_ (Olympia oyster)](http://en.wikipedia.org/wiki/Pacific_oyster) transcript counts stats in R - I mostly just watched and nodded. :)\n\n- Nextflow epidiverse/snp problems. Got to deal with these shenanigans:\n\n  ```shell\n  ================================================\n  E P I D I V E R S E - S N P    P I P E L I N E\n  ================================================\n  ~ version 1.0\n\n  input dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq\n  reference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081.fa\n  output dir    : snps\n  variant calls : enabled\n  clustering    : enabled\n\n  ================================================\n  RUN NAME: nostalgic_bhabha\n\n  executor >  local (34)\n  [d7/390a8d] process > SNPS:preprocessing (zr1394_all_s456_trimmed_bismark_bt2.deduplicated.sorted)     [100%] 10 of 10 ✔\n  [49/25512a] process > SNPS:masking (zr1394_11_s456_trimmed_bismark_bt2.deduplicated.sorted - variants) [ 60%] 24 of 40, failed: 24, retries: 20\n  [-        ] process > SNPS:extracting                                                                  -\n  [-        ] process > SNPS:khmer                                                                       -\n  [-        ] process > SNPS:kwip                                                                        -\n  [-        ] process > SNPS:clustering                                                                  -\n  [-        ] process > SNPS:sorting                                                                     -\n  [-        ] process > SNPS:freebayes                                                                   -\n  [-        ] process > SNPS:bcftools                                                                    -\n  [-        ] process > SNPS:plot_vcfstats                                                               -\n  Error executing process > 'SNPS:masking (zr1394_17_s456_trimmed_bismark_bt2.deduplicated.sorted - clustering)'\n\n  Caused by:\n    Process exceeded running time limit (4h)\n\n  Command executed:\n\n    change_sam_queries.py -Q -T 16 -t . -G calmd.bam clustering.bam || exit $?\n    find -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\n  Command exit status:\n    -\n\n  Command output:\n    (empty)\n\n  Command error:\n    WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\n\n  Work dir:\n    /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse/work/8b/636e263c8924aa275cd5f1fc7787e8\n\n  Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n  ```\n\n  - Updated custom config file (which was originally used solely to address permissions issues on the outptut files generated by the Docker container) to include improved time limits:\n\n\n    ```\n      docker {\n      fixOwnership = true\n    }\n\n\n    process {\n\n        // Process-specific resource requirements\n        withLabel:process_low {\n            time   = 30.d\n        }\n        withLabel:process_medium {\n            time   = 30.d\n        }\n        withLabel:process_high {\n            time   = 30.d\n        }\n        withLabel:process_long {\n            time   = 30.d\n        }\n    }\n    ```\n\n  - Seemed to fix timeout issue as this ran for nearly 6hrs without timing out error, but still getting the kernel memory warning in log files and constant failures at the masking step. Added this line to config file:\n\n    ```\n    docker.runOptions = \"--memory-swap '-1'\"\n    ```\n\n     Config file now looks like this ([grabbed idea to use this line to resolve kernel issue from here](https://github.com/nf-core/tools/pull/351)):\n\n     ```\n          docker {\n              docker.runOptions = \"--memory-swap '-1'\"\n              fixOwnership = true\n      }\n\n\n      process {\n\n          // Process-specific resource requirements\n          withLabel:process_low {\n              time   = 30.d\n          }\n          withLabel:process_medium {\n              time   = 30.d\n          }\n          withLabel:process_high {\n              time   = 30.d\n          }\n          withLabel:process_long {\n              time   = 30.d\n          }\n      }\n     ```\n---\n\n20220707\n\n- In order to start Nextflow epidiverse/snp, genome FastA file requires FastA index file; this is not documented, but pipeline throws an error when a FastA index is not found.\n\n- started Nextflow epidiverse/snp on raven\n\n---\n\n20220706\n\n- fixed [Nextflow epidiverse/snp problem](https://github.com/RobertsLab/resources/issues/1488) (GitHub Issue)\n\n- dealt with [lake trout BioProject PRJNA316738 data retrieval and QC](https://github.com/RobertsLab/resources/issues/1482) (GitHub Issue)\n\n---\n\n20220705\n\n- CEABIGR meeting:\n\n  - added gene names to transcript counts files \n  - added code to cover all desired iterations of output files (i.e. missing controls males files)\n\n","srcMarkdownNoYaml":"\n\n20220727\n\n- Ran qPCRs using 8 primer sets on Dorothy's cDNA from yesterday.\n\n---\n\n20220726\n\n- In lab to finish Dorothy's mussel RNA extraction and reverse transcription.\n\n\n---\n\n20220725\n\n- Read Ch. 7 of \"Fresh Banana Leaves\" for book club.\n\n- Lab meeting discussing Ch. 7 of \"Fresh Banana Leaves.\"\n\n---\n\n20220721\n\n- More testing with `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines (conda environments). This time ran trimming with an additional 5' 10bp hard trim, per [`Bismark`](https://github.com/FelixKrueger/Bismark) recommendations (they actually recommend 8bp).\n\n- Did a _lot_ of reading about Nextflow and trying to get a grasp on how it all works so we might be able to develop our own pipelines and/or feel confident modifying existing pipelines (e.g. modify `EpiDiverse/snp` pipeline to analyze all BAMs in a directory without the need to explicitly declare how many at run time).\n\n---\n\n20220720\n\n- More testing with `EpiDiverse/wgbs` pipeline. Per [this GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/11), I tried using the conda test profile and things ran smoothly without any errors in the `bam_statistics` part of the pipeline. Steps:\n\n1. Create conda environment:\n\n```shell\nconda env create --name epidiverse-wgbs-current -f /home/shared/epidiverse-pipelines/wgbs-current\n```\n\n2. Activate conda environment:\n\n```shell\nconda activate epdiverse-wgbs-current\n```\n\n3. Run conda test profile:\n\n```shell\nNXF_VER=20.07.1 /home/shared/nextflow run /home/shared/epidiverse-pipelines/wgbs-current -profile test,conda\n```\n\n- Did same conda test process for `EpiDiverse/snp` and it also ran without issue.\n\n- Started adding instructions for using Nextflow EpiDiverse pipelines on Raven and Mox to Roberts Lab Handbook.\n\n- Decided to re-run the `EpiDiverse/wgbs` pipeline using the conda environment and enable the FastQC process for post-trimming assessment. Will allow me to compare with our previous trimming results and try to determine why our trimming generates different results when passing our FastQs into the `EpiDiverse/snp` pipeline.\n\n---\n\n20220719\n\n- Assisted Dorothy with prep for big RNA extraction from mussel gills. Homogenized 10 control and 10 heat treated gill samples in 1mL RNAzol RT and stored @ -80C. Extraction to happen next Tuesday.\n\n---\n\n20220718\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220717\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220716\n\n- Ran some more tests of the `EpiDiverse/wgbs` and `EpiDiverse/snp` pipelines to compare the Oly trimmed and untrimmed FastQs as inputs.\n\n---\n\n20220715\n\n- Managed to resolve the [`EpiDiverse/wgbs` test issue I encountered yesterday](https://github.com/EpiDiverse/wgbs/issues/9). See the link for details (involves binding local directory to Singularity container).\n\n- Successfully ran the `EpiDiverse/wgbs` pipeline on a subset of the _Ostrea lurida_ BSseq data! Now to test this in the `EpiDiverse/snp` pipeline to see if output is same as when running Bismark BAMs through the `EpiDiverse/snp` pipeline...\n\n- Tested Bismark BAMs from different data set (_Panopea generosa_: https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/1231/) in `EpiDiverse/snp` pipeline to see how results compare to the _Ostrea lurida_ Bismark BAMs.\n\n---\n\n20220714\n\n- Installed Singularity on Raven (required Go, so installed that, too).\n\n- Tried to run `EpiDiverse/wgbs` test run on Raven, but it failed. See [the GitHub Issue I posted for deets.](https://github.com/EpiDiverse/wgbs/issues/9)\n\n---\n\n20220713\n\n- Got response to [my `EpiDiverse/snp` GitHub issue](https://github.com/EpiDiverse/snp/issues/6) regarding the pipeline not processing all BAM files in directory! Will re-ran pipeline and add the `--take <INTEGER>` parameter to handle all 18 BAM files.\n\n- Fixed [issue with CEABIGR transcript count stats](https://github.com/RobertsLab/resources/issues/1490). Original code assumed that calculations were only performed on input data and did not process newly added data generated by `mutate()` function. Fixed that by using the `-any_of()` fucntion to skip a vector of column names. The `-` preceding the `any_of()` is the instruction to _exclude_ things entered in the `any_of()` function. \n\n\n---\n\n20220712\n\n- In lab: walked Dorothy through the basics of RNA isolation (RNAzol and Direct-zol Mini Kit) and quantification (Qubit RNA BR Assay). Processed two fresh mussel ctenidia samples.\n\n---\n\n20220711\n\n- Lab meeting: Discussed Ch.6 of Fresh Banana Leaves\n\n- Oyster gene expression meeting: Steven did some plotting of [_Crassostrea virginica_ (Eastern oyster)](https://en.wikipedia.org/wiki/Eastern_oyster) transcript count data.\n\n- Nextflow `epidiverse/snp`: opted to try [a subsetted version of the Olurida_v081 genome and BSseq alignments](https://github.com/RobertsLab/resources/issues/1489#issuecomment-1179224402) (GitHub Issue):\n\nLooks like it's working (got past the previous hangups on 20220708)!\n\n```\nN E X T F L O W  ~  version 20.07.1\nLaunching `epidiverse/snp` [sick_hoover] - revision: 9c814703c6 [master]\n\n================================================\nE P I D I V E R S E - S N P    P I P E L I N E\n================================================\n~ version 1.0\n\ninput dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/bams/070322-olymerge-snp\nreference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081-mergecat98.fa\noutput dir    : snps\nvariant calls : enabled\nclustering    : enabled\n\n================================================\nRUN NAME: sick_hoover\n\nexecutor >  local (60)\n[2c/3853d0] process > SNPS:preprocessing (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n[90/dea51f] process > SNPS:masking (zr1394_9_s456_trimmed_bismark_bt2.deduplicated - clustering) [100%] 20 of 20 ✔\n[8a/ce2b44] process > SNPS:extracting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)           [100%] 10 of 10 ✔\n[01/ef9503] process > SNPS:khmer (zr1394_6_s456_trimmed_bismark_bt2.deduplicated)                [ 40%] 4 of 10\n[-        ] process > SNPS:kwip                                                                  -\n[-        ] process > SNPS:clustering                                                            -\n[83/238cfa] process > SNPS:sorting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)              [100%] 10 of 10 ✔\n[03/1b910d] process > SNPS:freebayes (zr1394_7_s456_trimmed_bismark_bt2.deduplicated)            [  0%] 0 of 10\n[-        ] process > SNPS:bcftools                                                              -\n[-        ] process > SNPS:plot_vcfstats \n```\n\nWell, well, well! It worked!!\n\n```\nN E X T F L O W  ~  version 20.07.1\nLaunching `epidiverse/snp` [sick_hoover] - revision: 9c814703c6 [master]\n\n================================================\nE P I D I V E R S E - S N P    P I P E L I N E\n================================================\n~ version 1.0\n\ninput dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/bams/070322-olymerge-snp\nreference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081-mergecat98.fa\noutput dir    : snps\nvariant calls : enabled\nclustering    : enabled\n\n================================================\nRUN NAME: sick_hoover\n\nexecutor >  local (92)\n[2c/3853d0] process > SNPS:preprocessing (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n[90/dea51f] process > SNPS:masking (zr1394_9_s456_trimmed_bismark_bt2.deduplicated - clustering) [100%] 20 of 20 ✔\n[8a/ce2b44] process > SNPS:extracting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)           [100%] 10 of 10 ✔\n[a4/dc11ce] process > SNPS:khmer (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)                [100%] 10 of 10 ✔\n[95/79be9f] process > SNPS:kwip                                                                  [100%] 1 of 1 ✔\n[c6/24b89b] process > SNPS:clustering                                                            [100%] 1 of 1 ✔\n[83/238cfa] process > SNPS:sorting (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)              [100%] 10 of 10 ✔\n[63/2c5bc5] process > SNPS:freebayes (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)            [100%] 10 of 10 ✔\n[00/f4af8b] process > SNPS:bcftools (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)             [100%] 10 of 10 ✔\n[df/1e78de] process > SNPS:plot_vcfstats (zr1394_9_s456_trimmed_bismark_bt2.deduplicated)        [100%] 10 of 10 ✔\n\nPipeline execution summary\n---------------------------\nName         : sick_hoover\nProfile      : docker\nLaunch dir   : /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse\nWork dir     : /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse/work (cleared)\nStatus       : success\nError report : -\n\nCompleted at: 11-Jul-2022 18:47:04\nDuration    : 3h 17m 49s\nCPU hours   : 140.1\nSucceeded   : 92\n\n```\n\nI'll obviously share this with Steven, but will also see if I can get the original dataset to run on Mox, which has twice the memory available as Raven...\n\nActually, I glanced at the data and it didn't really work. It only seems to have analyzed some of the BAMs in the directory! There are 18 BAMs, but it only processed 10 of them.. Odd (and annoying). I'll re-run to see if it does the same thing.\n\n\n---\n\n20220710\n\n- Created `erne-bs5` genome index to attempt full epidiverse pipeline, starting with `EpiDiverse/wgbs`:\n\n   ```shell\n/home/shared/erne-2.1.1-linux/bin/erne-create \\\n--methyl-hash \\\n--fasta Olurida_v081.fa \\\n--output-prefix Olurida_v081\n   ```\n- Realized I had experienced previous issues (see this [GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/5) and this [GitHub Issue](https://github.com/EpiDiverse/wgbs/issues/6)) previously, which is not encouraging.\n\n  Tried to run the test protocol for this pipeline \"locally\" (i.e. offline) using Mox sbatch script. To do so, I've downloaded all the of the input files listed in [`test.config`](https://github.com/EpiDiverse/wgbs/blob/master/config/test.config). I've also downloaded the Singularity image (`singularity pull docker://epidiverse/wgbs:1.0`) and changed the `nextflow.config` file to specify the Singularity image location, like so:\n\n```\n// -profile singularity\n\tsingularity {\n\t\tincludeConfig \"${baseDir}/config/base.config\"\n\t\tsingularity.enabled = true\n\t\tprocess.container = '/gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/singularity/wgbs_1.0.sif'\n\t}\n```\n\nThat seemed like that should be all that was needed, but when I execute the test command (`NXF_VER=20.07.1 /gscratch/srlab/programs/nextflow-21.10.6-all run /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/wgbs-1.0 -profile test,singularity`), it fails with this error:\n\n```\nexecutor >  local (10)\n[c4/79070c] process > INDEX:erne_bs5_indexing        [100%] 1 of 1 ✔\n[30/202688] process > INDEX:segemehl_indexing        [100%] 1 of 1 ✔\n[07/dc2230] process > WGBS:read_trimming (sampleB)   [100%] 8 of 8, failed: 8...\n[-        ] process > WGBS:read_merging              -\n[-        ] process > WGBS:fastqc                    -\n[-        ] process > WGBS:erne_bs5                  -\n[-        ] process > WGBS:segemehl                  -\n[-        ] process > WGBS:erne_bs5_processing       -\n[-        ] process > WGBS:segemehl_processing       -\n[-        ] process > WGBS:bam_merging               -\n[-        ] process > WGBS:bam_subsetting            -\n[-        ] process > WGBS:bam_filtering             -\n[-        ] process > WGBS:bam_statistics            -\n[-        ] process > CALL:bam_processing            -\n[-        ] process > CALL:Picard_MarkDuplicates     -\n[-        ] process > CALL:MethylDackel              -\n[-        ] process > CALL:conversion_rate_estima... -\n\nPipeline execution summary\n---------------------------\nName         : infallible_mccarthy\nProfile      : test,singularity\nLaunch dir   : /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test\nWork dir     : /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work\nStatus       : failed\nError report : Error executing process > 'WGBS:read_trimming (sampleA)'\n\nCaused by:\n  Process `WGBS:read_trimming (sampleA)` terminated with an error exit status (1)\n\nCommand executed:\n\n  mkdir fastq fastq/logs\n  cutadapt -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC \\\n  -q 20 -m 36 -O 3 \\\n  -o fastq/merge.null \\\n  -p fastq/merge.g null g \\\n  > fastq/logs/cutadapt.sampleA.merge.log 2>&1\n\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n\nWork dir:\n  /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/12/6ee9cc7a7372a97f34f21a4f79efb3\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\nError executing process > 'WGBS:read_trimming (sampleA)'\n\nCaused by:\n  Process `WGBS:read_trimming (sampleA)` terminated with an error exit status (1)\n\nCommand executed:\n\n  mkdir fastq fastq/logs\n  cutadapt -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC \\\n  -q 20 -m 36 -O 3 \\\n  -o fastq/merge.null \\\n  -p fastq/merge.g null g \\\n  > fastq/logs/cutadapt.sampleA.merge.log 2>&1\n\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n\nWork dir:\n  /gscratch/srlab/sam/analyses/20220710-olu-epidiverse_wgbs-test/work/12/6ee9cc7a7372a97f34f21a4f79efb3\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n```\n\nWhen I look at the Cutadapt log file, this is what is shown:\n\n\n```\ncat cutadapt.sampleA.merge.log \nThis is cutadapt 2.10 with Python 3.6.7\nCommand line parameters: -j 2 -a AGATCGGAAGAGC -A AGATCGGAAGAGC -q 20 -m 36 -O 3 -o fastq/merge.null -p fastq/merge.g null g\nProcessing reads on 2 cores in paired-end mode ...\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 477, in run\n    with xopen(self.file, 'rb') as f:\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/xopen/__init__.py\", line 407, in xopen\n    return open(filename, mode)\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 477, in run\n    with xopen(self.file, 'rb') as f:\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/xopen/__init__.py\", line 407, in xopen\n    return open(filename, mode)\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nERROR: Traceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 540, in run\n    raise e\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/wgbs/bin/cutadapt\", line 10, in <module>\n    sys.exit(main())\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/__main__.py\", line 855, in main\n    stats = r.run()\n  File \"/opt/conda/envs/wgbs/lib/python3.6/site-packages/cutadapt/pipeline.py\", line 770, in run\n    raise e\nIsADirectoryError: [Errno 21] Is a directory: 'null'\n```\n\nI [posted an Issue](https://github.com/EpiDiverse/wgbs/issues/8), but I'm not really expecting to get a response.\n\n---\n\n20220709\n\n- Nextflow epidiverse/snp: Let it run overnight and woke up to the same timeout error message and kernel memory warnings, despite changes in config file.. Although, I had been running it with the `-resume` flag... I'll remove all previous `snps/` and `work/` dirs and run with the following command:\n\n  ```shell\n  NXF_VER=20.07.1 /home/shared/nextflow run epidiverse/snp \\\n  -c nextflow-docker_permissions.config \\\n  -profile docker \\\n  --input /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq/ \\\n  --reference /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081.fa\n  ```\n\n  - Well that didn't work... I'm going to try again with a limited data set, as the `Olurida_v081.fa` has a TON of contigs, which has caused issues with other bisfulfite analysis software.\n\n---\n\n20220708\n\n- Spent time at Science Hour with Steven visualizing some of the [_Ostrea lurida_ (Olympia oyster)](http://en.wikipedia.org/wiki/Pacific_oyster) transcript counts stats in R - I mostly just watched and nodded. :)\n\n- Nextflow epidiverse/snp problems. Got to deal with these shenanigans:\n\n  ```shell\n  ================================================\n  E P I D I V E R S E - S N P    P I P E L I N E\n  ================================================\n  ~ version 1.0\n\n  input dir     : /home/shared/8TB_HDD_01/sam/data/O_lurida/BSseq\n  reference     : /home/shared/8TB_HDD_01/sam/data/O_lurida/genomes/Olurida_v081.fa\n  output dir    : snps\n  variant calls : enabled\n  clustering    : enabled\n\n  ================================================\n  RUN NAME: nostalgic_bhabha\n\n  executor >  local (34)\n  [d7/390a8d] process > SNPS:preprocessing (zr1394_all_s456_trimmed_bismark_bt2.deduplicated.sorted)     [100%] 10 of 10 ✔\n  [49/25512a] process > SNPS:masking (zr1394_11_s456_trimmed_bismark_bt2.deduplicated.sorted - variants) [ 60%] 24 of 40, failed: 24, retries: 20\n  [-        ] process > SNPS:extracting                                                                  -\n  [-        ] process > SNPS:khmer                                                                       -\n  [-        ] process > SNPS:kwip                                                                        -\n  [-        ] process > SNPS:clustering                                                                  -\n  [-        ] process > SNPS:sorting                                                                     -\n  [-        ] process > SNPS:freebayes                                                                   -\n  [-        ] process > SNPS:bcftools                                                                    -\n  [-        ] process > SNPS:plot_vcfstats                                                               -\n  Error executing process > 'SNPS:masking (zr1394_17_s456_trimmed_bismark_bt2.deduplicated.sorted - clustering)'\n\n  Caused by:\n    Process exceeded running time limit (4h)\n\n  Command executed:\n\n    change_sam_queries.py -Q -T 16 -t . -G calmd.bam clustering.bam || exit $?\n    find -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\n  Command exit status:\n    -\n\n  Command output:\n    (empty)\n\n  Command error:\n    WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\n\n  Work dir:\n    /home/shared/8TB_HDD_01/sam/analyses/20220707-olur-epidiverse/work/8b/636e263c8924aa275cd5f1fc7787e8\n\n  Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n  ```\n\n  - Updated custom config file (which was originally used solely to address permissions issues on the outptut files generated by the Docker container) to include improved time limits:\n\n\n    ```\n      docker {\n      fixOwnership = true\n    }\n\n\n    process {\n\n        // Process-specific resource requirements\n        withLabel:process_low {\n            time   = 30.d\n        }\n        withLabel:process_medium {\n            time   = 30.d\n        }\n        withLabel:process_high {\n            time   = 30.d\n        }\n        withLabel:process_long {\n            time   = 30.d\n        }\n    }\n    ```\n\n  - Seemed to fix timeout issue as this ran for nearly 6hrs without timing out error, but still getting the kernel memory warning in log files and constant failures at the masking step. Added this line to config file:\n\n    ```\n    docker.runOptions = \"--memory-swap '-1'\"\n    ```\n\n     Config file now looks like this ([grabbed idea to use this line to resolve kernel issue from here](https://github.com/nf-core/tools/pull/351)):\n\n     ```\n          docker {\n              docker.runOptions = \"--memory-swap '-1'\"\n              fixOwnership = true\n      }\n\n\n      process {\n\n          // Process-specific resource requirements\n          withLabel:process_low {\n              time   = 30.d\n          }\n          withLabel:process_medium {\n              time   = 30.d\n          }\n          withLabel:process_high {\n              time   = 30.d\n          }\n          withLabel:process_long {\n              time   = 30.d\n          }\n      }\n     ```\n---\n\n20220707\n\n- In order to start Nextflow epidiverse/snp, genome FastA file requires FastA index file; this is not documented, but pipeline throws an error when a FastA index is not found.\n\n- started Nextflow epidiverse/snp on raven\n\n---\n\n20220706\n\n- fixed [Nextflow epidiverse/snp problem](https://github.com/RobertsLab/resources/issues/1488) (GitHub Issue)\n\n- dealt with [lake trout BioProject PRJNA316738 data retrieval and QC](https://github.com/RobertsLab/resources/issues/1482) (GitHub Issue)\n\n---\n\n20220705\n\n- CEABIGR meeting:\n\n  - added gene names to transcript counts files \n  - added code to cover all desired iterations of output files (i.e. missing controls males files)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"toc-depth":5,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"source","theme":"darkly","page-layout":"full","code-background":true,"code-copy":true,"title-block-banner":true,"author":"Sam White","toc-title":"Contents","toc-location":"left","layout":"post","title":"Daily Bits - July 2022","date":"2022-07-05 09:49","tags":["daily bits","July"],"categories":["2022","Daily Bits"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}